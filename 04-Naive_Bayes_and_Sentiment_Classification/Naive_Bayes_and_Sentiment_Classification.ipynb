{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Sentiment Classification\n",
    "\n",
    "Classifier for text classification\n",
    "\n",
    "- Input: $d$ (\"document\")\n",
    "- Output: $c$ (\"class\")\n",
    "- Training set: $N$ documents that have each been hand-labeled with a class $(d_1, c_1), \\dots, (d_N, c_N)$\n",
    "\n",
    "- 🎯 Goal: to learn a classifier that is capable of mapping from a new document $d$ to its correct class $c\\in C$\n",
    "\n",
    "- Type:\n",
    "  - Generative: \n",
    "    - Build a model of how a class could generate some input data\n",
    "    - Given an observation, return the class most likely to have generated the observation.\n",
    "    - E.g.: Naive Bayes\n",
    "  - Discriminative\n",
    "    - Learn what features from the input are most useful to discriminate between the different possible classes\n",
    "    - Often more accurate and hence more commonly used\n",
    "    - E.g.: Logistic Regression\n",
    "\n",
    "\n",
    "\n",
    "## Naive Bayes Classifier\n",
    "\n",
    "We represent a text document as if were a **bag-of-words**\n",
    "\n",
    "- unordered set of words\n",
    "- position ignored\n",
    "- keeping only their frequency in the document\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/截屏2020-06-14%2011.55.44.png\" alt=\"截屏2020-06-14 11.55.44\" style=\"zoom:80%;\" />\n",
    "\n",
    "> In the example in the figure, instead of representing the word order in all the phrases like “I love this movie” and “I would recommend it”, we simply note that the word *I* occurred 5 times in the entire excerpt, the word *it* 6 times, the words *love*, *recommend*, and *movie* once, and so on.\n",
    "\n",
    "Naive Bayes:\n",
    "\n",
    "- Probablistic classifier \n",
    "\n",
    "  - for a document $d$, out of all classes $c \\in C$, the classifier returns the class $\\hat{c}$ which has the **maximum a-posterior probability (MAP)** given the document\n",
    "\n",
    "  $$\n",
    "  \\begin{array}{ll}\n",
    "  \\hat{c} &= \\underset{c \\in C}{\\operatorname{argmax}} P(c | d) \\\\\n",
    "  &\\overset{\\text{Bayesian inference}}{=} \\underset{c \\in C}{\\operatorname{argmax}} \\frac{P(d | c) P(c)}{P(d)} \\\\\n",
    "  &= \\underset{c \\in C}{\\operatorname{argmax}} \\underbrace{P(d | c)}_{\\text{likelihood}} \\underbrace{P(c)}_{\\text{prior}} \\\\\n",
    "  \n",
    "  \\end{array}\n",
    "  $$\n",
    "\n",
    "We represent a document $d$ as a set of features $f_1, f_2, \\dots, f_n$, then\n",
    "$$\n",
    "\\hat{c}=\\underset{c \\in C}{\\operatorname{argmax}} \\overbrace{P\\left(f_{1}, f_{2}, \\ldots, f_{n} | c\\right)}^{\\text {likelihood }} \\overbrace{P(c)}^{\\text {prior }}\n",
    "$$\n",
    "In order to make it possible to compute directly, Naive Bayes classifiers make two simplifying assumptions\n",
    "\n",
    "- Bag-of-words assumption\n",
    "  -  we assume position does NOT matter, and that the word “love” has the SAME effect on classification whether it occurs as the 1st, 20th, or last word in the document. \n",
    "  - Thus we assume that the features $f_1, f_2, \\dots, f_n$ only encode word identity and not position.\n",
    "\n",
    "- Naive Bayes assumption\n",
    "\n",
    "  - The conditional independence assumption that the probabilities $P(f_i|c)$ are independent given the class $c$ and hence can be ‘naively’ multiplied\n",
    "    $$\n",
    "    P\\left(f_{1}, f_{2}, \\ldots ., f_{n} | c\\right)=P\\left(f_{1} | c\\right) \\cdot P\\left(f_{2} | c\\right) \\cdot \\ldots \\cdot P\\left(f_{n} | c\\right)\n",
    "    $$\n",
    "\n",
    "Based on these two assumptions, the final equation for the class chosen by a naive Bayes classifier is thus\n",
    "$$\n",
    "c_{N B}=\\underset{c \\in C}{\\operatorname{argmax}} P(c) \\prod_{f \\in F} P(f | c)\n",
    "$$\n",
    "To apply the naive Bayes classifier to text, we need to consider word positions, by simply walking an index through every word position in the document:\n",
    "$$\n",
    "\\text{positions} \\leftarrow \\text{all word positions in test document}\\\\\n",
    "c_{NB}=\\underset{c \\in C}{\\operatorname{argmax}} P(c) \\prod_{i \\in \\text {positions}} P\\left(w_{i} | c\\right)\n",
    "$$\n",
    "To avoid underflow and increase speed, Naive Bayes calculations, like calculations for language modeling, are done in log space:\n",
    "$$\n",
    "c_{NB}=\\underset{c \\in C}{\\operatorname{argmax}} \\log P(c)+\\sum_{i \\in  \\text {postiions }} \\log P\\left(w_{i} | c\\right)\n",
    "\\label{eq:NB log}\n",
    "$$\n",
    "\n",
    "\n",
    "## Training the Naive Bayes Classifier\n",
    "\n",
    "In $\\eqref{eq:NB log}$ we have to learn the probabilities $P(c)$ and $P(w_i|c)$. We use the **Maximum Likelihood Estimate (MLE)** to estimate them. We’ll simply use the **frequencies** in the data.\n",
    "\n",
    "- $P(c)$: document prior\n",
    "\n",
    "  - \"what percentage of the documents in our training set are in each class $c$ ?\"\n",
    "\n",
    "  $$\n",
    "  \\hat{P}(c)=\\frac{N_{c}}{N_{d o c}}\n",
    "  $$\n",
    "\n",
    "  - $N_c$: the number of documents in our training data with class $c$\n",
    "  - $N_{doc}$: the total number of documents\n",
    "\n",
    "- $P(w_i|c)$\n",
    "\n",
    "  - \"The fraction of times the word $w_i$ appears among all words in all documents of topic $c$\"\n",
    "\n",
    "    - We first concatenate all documents with category $c$ into one big “category $c$” text.\n",
    "\n",
    "    - Then we use the frequency of $w_i$ in this concatenated document to give a maximum likelihood estimate of the probability\n",
    "      $$\n",
    "      \\hat{P}\\left(w_{i} | c\\right)=\\frac{\\operatorname{count}\\left(w_{i}, c\\right)}{\\sum_{w \\in V} \\operatorname{count}(w, c)}\n",
    "      $$\n",
    "\n",
    "      - $V$: vocabulary that consists of the union of all the word types in all classes, not just the words in one class $c$\n",
    "\n",
    "  - Avoid zero probablities in the likelihood term for any class: **Laplace (add-one) smoothing**\n",
    "    $$\n",
    "    \\hat{P}\\left(w_{i} | c\\right)=\\frac{\\operatorname{count}\\left(w_{i}, c\\right)+1}{\\sum_{w \\in V}(\\operatorname{count}(w, c)+1)}=\\frac{\\operatorname{count}\\left(w_{i}, c\\right)+1}{\\left(\\sum_{w \\in V} \\operatorname{count}(w, c)\\right)+|V|}\n",
    "    $$\n",
    "\n",
    "    > Why is this a problem?\n",
    "    >\n",
    "    > Imagine we are trying to estimate the likelihood of the word “fantastic” given class *positive*, but suppose there are no training documents that both contain the word “fantastic” and are classified as *positive*. Perhaps the word “fantastic” happens to occur (sarcastically?) in the class *negative*. In such a case the probability for this feature will be zero:\n",
    "    > $$\n",
    "    > \\hat{P}(\\text { \"fantastic\" } | \\text { positive })=\\frac{\\text { count }(\\text { \"fantastic } \" \\text { , positive })}{\\sum_{w \\in V} \\text { count }(w, \\text { positive })}=0\n",
    "    > $$\n",
    "    > But since naive Bayes naively multiplies all the feature likelihoods together, zero probabilities in the likelihood term for any class will cause the probability of the class to be zero, no matter the other evidence!\n",
    "\n",
    "In addition to $P(c)$ and $P(w_i|c)$, We should also deal with\n",
    "\n",
    "- **Unknown words** \n",
    "  - Words that occur in our test data but are NOT in our vocabulary at all because they did not occur in any training document in any class\n",
    "  - 🔧 Solution: Ignore them\n",
    "    - Remove them from the test document and not include any probability for them at all\n",
    "- **Stop words**\n",
    "  - Very frequent words like *the* and *a*\n",
    "  - Solution:\n",
    "    - Method 1: \n",
    "      1. sorting the vocabulary by frequency in the training set\n",
    "      2. defining the top 10–100 vocabulary entries as stop words\n",
    "    - Method 2: \n",
    "      1. use one of the many pre-defined stop word list available online\n",
    "      2. Then every instance of these stop words are simply removed from both training and test documents as if they had never occurred.\n",
    "  - In most text classification applications, however, using a stop word list does NOT improve performance 🤪, and so it is more common to make use of the entire vocabulary and not use a stop word list.\n",
    "\n",
    "### Example\n",
    "\n",
    "We’ll use a sentiment analysis domain with the two classes positive (+) and negative (-), and take the following miniature training and test documents simplified from actual movie reviews.\n",
    "\n",
    "![截屏2020-06-14 12.37.43](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/截屏2020-06-14%2012.37.43.png)\n",
    "\n",
    "- The prior $P(c)$:\n",
    "  $$\n",
    "  P(-)=\\frac{3}{5} \\quad P(+)=\\frac{2}{5}\n",
    "  $$\n",
    "\n",
    "The word *with* doesn’t occur in the training set, so we drop it completely.\n",
    "\n",
    "The remaining three words are \"predictable\", \"no\", and \"fun\". Their likelihoods from the training set are (with Laplace smoothing): \n",
    "$$\n",
    "\\begin{aligned}\n",
    "P\\left(\\text { \"predictable\" }|-\\right) &=\\frac{1+1}{14+20} \\quad P\\left(\\text { \"predictable\" } |+\\right)=\\frac{0+1}{9+20} \\\\\n",
    "P\\left(\\text{\"no\"}|-\\right) &=\\frac{1+1}{14+20} \\quad P\\left(\\text{\"no\"}|+\\right)=\\frac{0+1}{9+20} \\\\\n",
    "P\\left(\\text{\"fun\"}|-\\right) &=\\frac{0+1}{14+20} \\quad P\\left(\\text{\"fun\"}|+\\right)=\\frac{1+1}{9+20}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> $$\n",
    "> V=\\{\\text{just, plain, boring, entirely, predictable, and, lacks, energy, no, suprises, very, few, laughs, powerful, the, most, fun, film, of, summer}\\}\n",
    "> $$\n",
    ">\n",
    "> $\\Rightarrow |V|=20$\n",
    ">\n",
    "> The word \"predictable\" occurs in negative (-) once, therefore, with Laplace smoothing:\n",
    ">\n",
    "> $P\\left(\\text { \"predictable\" }|-\\right) =\\frac{1+1}{14+20}$\n",
    "\n",
    "For the test sentence $S=$ \"predictable with no fun\", after removing the word \"with\":\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "P(-) P(S |-)=\\frac{3}{5} \\times \\frac{2 \\times 2 \\times 1}{34^{3}}=6.1 \\times 10^{-5} \\\\\n",
    "P(+) P(S |+)=\\frac{2}{5} \\times \\frac{1 \\times 1 \\times 2}{29^{3}}=3.2 \\times 10^{-5}\n",
    "\\end{array}\n",
    "$$\n",
    "$P(-)P(S|-) > P(+)P(S|+)$ \n",
    "\n",
    "$\\Rightarrow$ The model predicts the class *negative* for the test sentence $S$.\n",
    "\n",
    "\n",
    "\n",
    "## Optimizing for Sentiment Analysis\n",
    "\n",
    "While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance. 💪\n",
    "\n",
    "### Binary multinomial naive Bayes (binary NB)\n",
    "\n",
    "First, for sentiment classification and a number of other text classification tasks, whether a word occurs or not seems to matter more than its frequency. \n",
    "\n",
    "**Thus it often improves performance to clip the word counts in each document at 1**.\n",
    "\n",
    "Example:\n",
    "\n",
    "- The example is worked without add-1 smoothing to make the differences clearer. \n",
    "- Note that the results counts need not be 1; the word *great* has a count of 2 even for Binary NB, because it appears in multiple documents (in both positive and negative class).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/截屏2020-06-14%2012.55.28.png\" alt=\"截屏2020-06-14 12.55.28\" style=\"zoom:80%;\" />\n",
    "\n",
    "### Deal with negation\n",
    "\n",
    "During text normalization, prepend the prefix *NOT_* to every word after a token of logical negation (*n’t, not, no, never*) until the next punc- tuation mark. \n",
    "\n",
    "Example:\n",
    "\n",
    "![截屏2020-06-14 12.58.03](/Users/EckoTan/Library/Application Support/typora-user-images/截屏2020-06-14 12.58.03.png)\n",
    "\n",
    "### Deal with insufficient labeled training data\n",
    "\n",
    "Derive the positive and negative word features from sentiment lexicons, lists of words that are pre-annotated with positive or negative sentiment.\n",
    "\n",
    "Popular lexicons:\n",
    "\n",
    "- General Inquirer\n",
    "- LIWC\n",
    "- opinion lexicon\n",
    "- MPQA Subjectivity Lexicon\n",
    "\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "**Gold labels**: the human-defined labels for each document that we are trying to match\n",
    "\n",
    "To evaluate any system for detecting things, we start by building a **Contingency table (Confusion matrix)**:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/截屏2020-06-14%2013.42.20.png\" alt=\"截屏2020-06-14 13.42.20\" style=\"zoom:80%;\" />\n",
    "\n",
    "Evaluation metric:\n",
    "\n",
    "- **Accuracy**\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\text{tp+tn}}{\\text{tp+fp+tn+fn}}\n",
    "  $$\n",
    "\n",
    "  - doesn’t work well when the classes are unbalanced 🤪\n",
    "\n",
    "- **Precision (P)**\n",
    "\n",
    "  - measures the percentage of the items that the system detected (i.e., the system labeled as positive) that are in fact positive (i.e., are positive according to the human gold labels).\n",
    "    $$\n",
    "    \\text{Precision} = \\frac{\\text{tp}}{\\text{tp+fp}}\n",
    "    $$\n",
    "\n",
    "- **Recall (R)**\n",
    "\n",
    "  - measures the percentage of items actually present in the input (i.e., are positive according to the human gold labels) that were correctly identified by the system (i.e., the system labeled as positive).\n",
    "    $$\n",
    "    \\text{Recall} = \\frac{\\text{tp}}{\\text{tp+fn}}\n",
    "    $$\n",
    "\n",
    "- **F-measure**\n",
    "  $$\n",
    "  F_{\\beta}=\\frac{\\left(\\beta^{2}+1\\right) P R}{\\beta^{2} P+R}\n",
    "  $$\n",
    "\n",
    "  - $\\beta$: differentially weights the importance of recall and precision, based perhaps on the needs of an application\n",
    "\n",
    "    - $\\beta > 1$: favor recall\n",
    "\n",
    "    - $\\beta < 1$: favor precision\n",
    "\n",
    "    - $\\beta = 1$: precision and recall are equally balanced (the most frequently used metric)\n",
    "      $$\n",
    "      F_{1}=\\frac{2 P R}{P+R}\n",
    "      $$\n",
    "\n",
    "### More than two classes\n",
    "\n",
    "Solution: **one-of** or **multinomial classification**\n",
    "\n",
    "- The classes are **mutually exclusive** and each document or item appears in exactly ONE class.\n",
    "- How it works?\n",
    "  - We build a separate binary classifier trained on positive examples from $c$ and negative examples from all other classes. \n",
    "  - Given a test document or item $d$, we run all the classifiers and choose the label from the classifier with the highest score.\n",
    "\n",
    "E.g.:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/截屏2020-06-14%2013.58.26.png\" alt=\"截屏2020-06-14 13.58.26\" style=\"zoom:80%;\" />\n",
    "\n",
    "Evaluation metric:\n",
    "\n",
    "- **Macroaveraging**\n",
    "  1. compute the performance for each class\n",
    "  2. then average over classes\n",
    "- **Microaveraging**\n",
    "  1. collect the decisions for all classes into a single contingency table\n",
    "  2. then compute precision and recall from that table.\n",
    "\n",
    "E.g.: \n",
    "\n",
    "![截屏2020-06-14 14.00.23](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/截屏2020-06-14%2014.00.23.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
