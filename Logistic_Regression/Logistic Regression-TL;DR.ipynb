{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (TL;DR)\n",
    "\n",
    "- Supervised classification\n",
    "\n",
    "- Input: $x = (x_1, x_2, \\dots, x_n)^T$\n",
    "\n",
    "- Output: $y \\in \\{0, 1\\}$\n",
    "\n",
    "- Parameters:\n",
    "\n",
    "  - Weight: $w = (w_1, w_2, \\dots, w_n)^T$\n",
    "  - Bias $b$\n",
    "\n",
    "- Prediction\n",
    "  $$\n",
    "  z = w \\cdot x + b \\\\\n",
    "  P(y=1|x)=\\sigma(z) = \\frac{1}{1+e^{-z}}\\\\\n",
    "  y=\\left\\{\\begin{array}{ll}\n",
    "  1 & \\text { if } P(y=1 | x)>0.5 \\\\\n",
    "  0 & \\text { otherwise }\n",
    "  \\end{array}\\right.\n",
    "  $$\n",
    "\n",
    "- Training/Learning\n",
    "\n",
    "  - Loss function\n",
    "\n",
    "    - For a single sample $x$\n",
    "      $$\n",
    "      \\hat{y} = \\sigma(w \\cdot x + b)\n",
    "      $$\n",
    "      And we define $\\hat{y}:=P(y=1|x)$\n",
    "\n",
    "      $y \\in \\{0, 1\\} \\Rightarrow$\n",
    "      $$\n",
    "      P(y | x)=\\left\\{\\begin{array}{lr}\n",
    "      \\hat{y} & y=1 \\\\\n",
    "      1-\\hat{y}  & y=0\n",
    "      \\end{array}\\right.\n",
    "      $$\n",
    "      The probability of correct prediction can thus be expressed as:\n",
    "      $$\n",
    "      P(y|x)=\\hat{y}^y (1-\\hat{y})^{1-y}\n",
    "      $$\n",
    "      We want to maximize $P(y|x)$\n",
    "      $$\n",
    "      \\begin{array}{ll}\n",
    "      &\\max \\quad P(y|x) \\\\\n",
    "      \\equiv &\\max \\quad \\log(P(y|x)) \\\\\n",
    "      = &\\max \\quad \\log(\\hat{y}^y (1-\\hat{y})^{1-y})\\\\\n",
    "      = &\\max \\quad y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}) \\\\\n",
    "      \\equiv &\\min \\quad -[y \\log \\hat{y}+(1-y) \\log (1-\\hat{y})] \\\\\n",
    "      = &\\min \\quad \\underbrace{-[y \\log \\sigma(w \\cdot x + b) + (1-y) \\log (1-\\sigma(w \\cdot x + b))]}_{=:L_{CE}(w, b)} \\\\\n",
    "      \\end{array}\n",
    "      $$\n",
    "      $L_{CE}(w, b)$ is called the **Cross-Entropy loss**.\n",
    "\n",
    "    - For a mini-batch of samples of size $m$\n",
    "\n",
    "      - $(x^{(i)}, y^{(i)})$: $i$-th Training sample\n",
    "\n",
    "      - Loss function is the **average loss** for each example\n",
    "        $$\n",
    "        L(w, b) = =-\\frac{1}{m} \\sum_{i=1}^{m} y^{(i)} \\log \\sigma\\left(w \\cdot x^{(i)}+b\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-\\sigma\\left(w \\cdot x^{(i)}+b\\right)\\right)\n",
    "        $$\n",
    "\n",
    "  - Algorithm: **Gradient descent**\n",
    "\n",
    "    - Gradient for single sample:\n",
    "      $$\n",
    "      \\frac{\\partial L_{C E}(w, b)}{\\partial w_{j}}=[\\sigma(w \\cdot x+b)-y] x_{j}\n",
    "      $$\n",
    "\n",
    "    - Gradient for mini-batch:\n",
    "      $$\n",
    "      \\frac{\\partial L(w, b)}{\\partial w_{j}}=\\frac{1}{m} \\sum_{i=1}^{m}\\left[\\sigma\\left(w \\cdot x^{(i)}+b\\right)-y^{(i)}\\right] x_{j}^{(i)}\n",
    "      $$\n",
    "\n",
    "      - $x_j^{(i)}$: $j$-th feature of the $i$-th sample\n",
    "\n",
    "\n",
    "\n",
    "## Multinomial Logistic Regression\n",
    "\n",
    "- Also called **Softmax regression**, **MaxEnt classifier**\n",
    "\n",
    "- Softmax function\n",
    "  $$\n",
    "  \\operatorname{softmax}\\left(z_{i}\\right)=\\frac{e^{z_{i}}}{\\sum_{j=1}^{k} e^{z_{j}}} \\qquad 1 \\leq i \\leq k\n",
    "  $$\n",
    "\n",
    "- Compute the probability of $y$ being in each potential class $c \\in C$, $p(y=c|x)$, using softmax function:\n",
    "  $$\n",
    "  p(y=c | x)=\\frac{e^{w_{c} \\cdot x+b_{c}}}{\\displaystyle\\sum_{j=1}^{k} e^{w_{j} \\cdot x+b_{j}}}\n",
    "  $$\n",
    "\n",
    "- Prediction\n",
    "  $$\n",
    "  \\begin{array}{ll}\n",
    "  \\hat{c} &= \\underset{c}{\\arg \\max} \\quad p(y=c | x) \\\\\n",
    "  &= \\underset{c}{\\arg \\max} \\quad \\frac{e^{w_{c} \\cdot x+b_{c}}}{\\displaystyle\\sum_{j=1}^{k} e^{w_{j} \\cdot x+b_{j}}}\n",
    "  \\end{array}\n",
    "  $$\n",
    "\n",
    "- Learning\n",
    "\n",
    "  - For a singnle sample $x$, the loss function is\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    L_{C E}(w, b) &=-\\sum_{k=1}^{K} 1\\{y=k\\} \\log p(y=k | x) \\\\\n",
    "    &=-\\sum_{k=1}^{K} 1\\{y=k\\} \\log \\frac{e^{w_{k} \\cdot x+b_{k}}}{\\sum_{j=1}^{K} e^{w_{j} \\cdot x+b_{j}}}\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "    - $1\\{\\}$: evaluates to $1$ if the condition in the brackets is true and to $0$ otherwise.\n",
    "\n",
    "  - Gradient\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\frac{\\partial L_{C E}}{\\partial w_{k}} &=-(1\\{y=k\\}-p(y=k | x)) x_{k} \\\\\n",
    "    &=-\\left(1\\{y=k\\}-\\frac{e^{w_{k} \\cdot x+b_{k}}}{\\sum_{j=1}^{K} e^{w_{j} \\cdot x+b_{j}}}\\right) x_{k}\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
