{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM POS Tagging\n",
    "\n",
    "**Sequence model/classifier**\n",
    "\n",
    "- Assign a label or class to each unit in a sequence\n",
    "- mapping a sequence of observation to a sequence of labels\n",
    "\n",
    "**Hidden Markov Model (HMM)** is a probabilistic sequence model\n",
    "\n",
    "- given a sequence of units (words, letters, morphemes, sentences, whatever)\n",
    "- it computes a probability distribution over possible sequences of labels and chooses the best label sequence\n",
    "\n",
    "\n",
    "\n",
    "## Markov Chains\n",
    "\n",
    "A **Markov chain** is a model that tells us something about the probabilities of sequences of random variables, ***states***, each of which can take on values from some set. These sets can be words, or tags, or symbols representing anything (E.g., *the weather*).\n",
    "\n",
    "üí° A Markov chain makes a very strong assumption that \n",
    "\n",
    "- if we want to predict the future in the sequence, all that matters is the current state. \n",
    "- All the states before the current state have NO impact on the future except via the current state.\n",
    "  - *It‚Äôs as if to predict tomorrow‚Äôs weather you could examine today‚Äôs weather but you weren‚Äôt allowed to look at yesterday‚Äôs weather.*\n",
    "\n",
    "üëÜ **Markov assumption**\n",
    "\n",
    "- Consider a sequence of state variables $q_1, q_2, \\dots, q_i$\n",
    "\n",
    "- When predicting the future, the past does NOT matter, only the present\n",
    "  $$\n",
    "  P\\left(q_{i}=a | q_{1} \\ldots q_{i-1}\\right)=P\\left(q_{i}=a | q_{i-1}\\right)\n",
    "  $$\n",
    "\n",
    "**Markov chain**\n",
    "\n",
    "- embodies the Markov assumption on the probabilities of this sequence\n",
    "\n",
    "- specified by the following components\n",
    "\n",
    "  - $Q = {q_1, q_2, \\dots, q_N}$: a set of $N$ **states**\n",
    "\n",
    "  - $A=a_{11} a_{12} \\dots a_{n 1} \\dots a_{n n}$: **transition probability matrix**\n",
    "\n",
    "    - Each $a_{ij}$ represents the probability of moving from state $i$ to state $j$, s.t.\n",
    "      $$\n",
    "      \\sum_{j=1}^{n} a_{i j}=1 \\quad \\forall i\n",
    "      $$\n",
    "\n",
    "  - $\\pi=\\pi_{1}, \\pi_{2}, \\dots, \\pi_{N}$: an **initial probability distribution** over states\n",
    "\n",
    "    - $\\pi_i$: probability that the Markov chain will start in state $i$\n",
    "    - Some states $j$ may have $\\pi_j = 0$ (meaning that the can NOT be initial states)\n",
    "    - $\\displaystyle\\sum_{i=1}^{n} \\pi_{i}=1$ \n",
    "\n",
    "- Example\n",
    "\n",
    "  ‚Äã\t![Êà™Â±è2020-05-23 18.01.29](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-23 18.01.29.png)\n",
    "\n",
    "  - Nodes: states\n",
    "\n",
    "  - Edges: transitions, with their probabilities\n",
    "\n",
    "    - The values of arcs leaving a given state must sum to 1.\n",
    "\n",
    "  - Setting start distribution $\\pi = [0.1, 0.7, 0.2]$ would mean a probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot)\n",
    "\n",
    "    \n",
    "\n",
    "## Hidden Markov Model (HMM)\n",
    "\n",
    "A Markov chain is useful when we need to compute a probability for a sequence of observable events. \n",
    "\n",
    "In many cases, however, the events we are interested in are **hidden**: we don‚Äôt observe them directly.\n",
    "\n",
    "- We do NOT normally observe POS tags in a text\n",
    "- Rather, we see words, and must infer the tags from the word sequence\n",
    "\n",
    "$\\Rightarrow$ We call the tags **hidden** because they are NOT observed.\n",
    "\n",
    "**Hidden Markov Model (HMM)**\n",
    "\n",
    "- allows us to talk about both *observed* events (like words that we see in the input) and *hidden* events (like part-of-speech tags) that we think of as causal factors in our probabilistic model\n",
    "\n",
    "- Specified by:\n",
    "\n",
    "  - $Q = {q_1, q_2, \\dots, q_N}$: a set of $N$ **states**\n",
    "\n",
    "  - $A=a_{11} a_{12} \\dots a_{n 1} \\dots a_{n n}$: **transition probability matrix**\n",
    "\n",
    "    - Each $a_{ij}$ represents the probability of moving from state $i$ to state $j$, s.t.\n",
    "      $$\n",
    "      \\sum_{j=1}^{n} a_{i j}=1 \\quad \\forall i\n",
    "      $$\n",
    "\n",
    "  - $O = {o_1, o_2, \\dots, o_T}$: a set of $T$ **observations**\n",
    "\n",
    "    - Each one drawn from a vocabulary $V = {v_1, v_2, \\dots, v_V}$\n",
    "\n",
    "  - $B=b_{i}\\left(o_{t}\\right)$: a sequence of **observation likelihoods** (also called **emission probabilities**) \n",
    "\n",
    "    - Each expressing the probability of an observation $o_t$ being generated from a state $q_i$\n",
    "\n",
    "  - $\\pi=\\pi_{1}, \\pi_{2}, \\dots, \\pi_{N}$: an **initial probability distribution** over states\n",
    "\n",
    "    - $\\pi_i$: probability that the Markov chain will start in state $i$\n",
    "    - Some states $j$ may have $\\pi_j = 0$ (meaning that the can NOT be initial states)\n",
    "    - $\\displaystyle\\sum_{i=1}^{n} \\pi_{i}=1$ \n",
    "\n",
    "- A first-order hidden Markov model instantiates two simplifying assumptions\n",
    "\n",
    "  - Markov assumption: the probability of a particular state depends only on the previous state \n",
    "    $$\n",
    "    P\\left(q_{i}=a | q_{1} \\ldots q_{i-1}\\right)=P\\left(q_{i}=a | q_{i-1}\\right)\n",
    "    $$\n",
    "\n",
    "  - Output independence: the probability of an output observation $o_i$ depends only on the state that produced the observation $q_i$ and NOT on any other states or any other observations \n",
    "    $$\n",
    "    P\\left(o_{i} | q_{1} \\ldots q_{i}, \\ldots, q_{T}, o_{1}, \\ldots, o_{i}, \\ldots, o_{T}\\right)=P\\left(o_{i} | q_{i}\\right)\n",
    "    $$\n",
    "\n",
    "\n",
    "\n",
    "## Components of HMM tagger\n",
    "\n",
    "An HMM has two components, the $A$ and $B$ probabilities\n",
    "\n",
    "The $A$ matrix contains the tag transition probabilities $P(t_i | t_{i-1})$ which represent the probability of a tag occurring given the previous tag.\n",
    "\n",
    "- E.g., modal verbs like *will* are very likely to be followed by a verb in the base form, a VB, like *race*, so we expect this probability to be high.\n",
    "\n",
    "We compute the maximum likelihood estimate of this transition probability by **counting**, out of the times we see the first tag in a labeled corpus, how often the first tag is followed by the second\n",
    "$$\n",
    "P\\left(t_{i} | t_{i-1}\\right)=\\frac{C\\left(t_{i-1}, t_{i}\\right)}{C\\left(t_{i-1}\\right)}\n",
    "$$\n",
    "\n",
    "- For example, in the WSJ corpus, MD occurs 13124 times of which it is followed by VB 10471. Therefore, for an MLE estimate of\n",
    "  $$\n",
    "  P(V B | M D)=\\frac{C(M D, V B)}{C(M D)}=\\frac{10471}{13124}=.80\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "The $B$ emission probabilities, $P(w_i|t_i)$,  represent the probability, given a tag (say MD), that it will be associated with a given word (say *will*). The MLE of the emission probability is \n",
    "$$\n",
    "P\\left(w_{i} | t_{i}\\right)=\\frac{C\\left(t_{i}, w_{i}\\right)}{C\\left(t_{i}\\right)}\n",
    "$$\n",
    "\n",
    "- E.g.: Of the 13124 occurrences of MD in the WSJ corpus, it is associated with *will* 4046 times\n",
    "  $$\n",
    "  P(w i l l | M D)=\\frac{C(M D, w i l l)}{C(M D)}=\\frac{4046}{13124}=.31\n",
    "  $$\n",
    "\n",
    "Note that this likelihood term is NOT asking ‚Äúwhich is the most likely tag for the word *will*?‚Äù  That would be the posterior $P(\\text{MD}|\\text{will})$. Instead, $P(\\text{will}|\\text{MD})$ answers the question ‚ÄúIf we were going to generate a MD, how likely is it that this modal would be *will*?‚Äù\n",
    "\n",
    "Example: three states HMM POS tagger\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-23%2022.17.43.png\" alt=\"Êà™Â±è2020-05-23 22.17.43\" style=\"zoom:50%;\" />\n",
    "\n",
    "\n",
    "\n",
    "## HMM tagging as decoding\n",
    "\n",
    "**Decoding**: Given as input an HMM $\\lambda = (A, B)$ and a sequence of observations $O = o_1, o_2, \\dots,o_T$, find the most probable sequence of states $Q = q_1q_2 \\dots q_T$\n",
    "\n",
    "üéØ For part-of-speech tagging, the goal of HMM decoding is to choose the tag sequence $t_1^n$ that is most probable given the observation sequence of $n$ words $w_1^n$\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\n",
    "\\hat{t}_{1}^{n} &= \\underset{t_{1}^{n}}{\\operatorname{argmax}} P\\left(t_{1}^{n} | w_{1}^{n}\\right) \\\\\n",
    "\n",
    "&\\overset{\\text{Bayes' rule}}{=} \\underset{t_{1}^{n}}{\\operatorname{argmax}} \\frac{P\\left(w_{1}^{n} | t_{1}^{n}\\right) P\\left(t_{1}^{n}\\right)}{P\\left(w_{1}^{n}\\right)} \\\\\n",
    "\n",
    "&=\\underset{t_{1}^{n}}{\\operatorname{argmax}} P\\left(w_{1}^{n} | t_{1}^{n}\\right) P\\left(t_{1}^{n}\\right)\n",
    "\n",
    "\n",
    "\\end{array}\n",
    "$$\n",
    "HMM taggers make two simplifying assumptions:\n",
    "\n",
    "- the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags\n",
    "  $$\n",
    "  P\\left(w_{1}^{n} | t_{1}^{n}\\right) \\approx \\prod_{i=1}^{n} P\\left(w_{i} | t_{i}\\right)\n",
    "  $$\n",
    "\n",
    "- the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence (the **bigram** assumption)\n",
    "  $$\n",
    "  P\\left(t_{1}^{n}\\right) \\approx \\prod_{i=1}^{n} P\\left(t_{i} | t_{i-1}\\right)\n",
    "  $$\n",
    "\n",
    "Combing these two assumptions, the most probable tag sequence from a bigram tagger is:\n",
    "$$\n",
    "\\hat{t}_{1}^{n}=\\underset{t_{1}^{n}}{\\operatorname{argmax}} P\\left(t_{1}^{n} | w_{1}^{n}\\right) \\approx \\underset{t_{1}^{n}}{\\operatorname{argmax}} \\prod_{i=1}^{n} \\overbrace{P\\left(w_{i} | t_{i}\\right)}^{\\text {emission}} \\cdot \\overbrace{P\\left(t_{i} | t_{i-1}\\right)}^{\\text{transition }}\n",
    "$$\n",
    "\n",
    "\n",
    "## The Viterbi Algorithm\n",
    "\n",
    "The Viterbi algorithm:\n",
    "\n",
    "- Decoding algorithm for HMMs\n",
    "- An instance of dynamic programming\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-24%2011.45.05.png\" alt=\"Êà™Â±è2020-05-24 11.45.05\" style=\"zoom:50%;\" />\n",
    "\n",
    "The Viterbi algorithm first sets up a probability matrix or **lattice**\n",
    "\n",
    "- One column for each observation $o_t$\n",
    "\n",
    "- One row for each state in the state graph\n",
    "\n",
    "  $\\rightarrow$ Each column has a cell for each state $q_i$ in the single combined automation\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-24%2011.49.16.png\" alt=\"Êà™Â±è2020-05-24 11.49.16\" style=\"zoom:40%;\" />\n",
    "\n",
    "Each cell of the lattice $v_t(j)$\n",
    "\n",
    "- represents the probability that the HMM is in state $j$ after seeing the first $t$ observatins and passing through the most probable state sequence $q_1, \\dots, q_{t-1}$, given the HMM $\\lambda$\n",
    "\n",
    "- The value of each cell $v_t(j)$ is computed by recursively taking **the most probable** path that could lead us to this cell\n",
    "  $$\n",
    "  v_{t}(j)=\\max _{q_{1}, \\ldots, q_{t-1}} P\\left(q_{1} \\ldots q_{t-1}, o_{1}, o_{2} \\ldots o_{t}, q_{t}=j | \\lambda\\right)\n",
    "  $$\n",
    "\n",
    "  - Represent the most probable path by taking the maximum over all possible previous state sequences $\\underset{{q_{1}, \\ldots, q_{t-1}}}{\\max}$\n",
    "\n",
    "    - Viterbi fills each cell recursively (like other dynamic programming algorithms)\n",
    "\n",
    "    - Given that we had already computed the probability of being in every state at time $t-1$, we compute the Viterbi probability by taking the most probable of the extensions of the paths that lead to the current cell. \n",
    "\n",
    "      For a given state $q_j$ at time $t$, the value $v_t(j)$ is computed as\n",
    "      $$\n",
    "      v_{t}(j)=\\max _{i=1}^{N} v_{t-1}(i) a_{i j} b_{j}\\left(o_{t}\\right)\n",
    "      $$\n",
    "\n",
    "      - $v_{t-1}(i)$: the **previous Viterbi path** probability from the previous time step\n",
    "      - $a_{ij}$: the **transition probability** from previous state $q_i$ to current state $q_j$\n",
    "      - $b_j(o_t)$: the **state observation likelihood** of the observation symbol $o_t$ given the current state $j$\n",
    "\n",
    "### Example 1\n",
    "\n",
    "Tag the sentence \"*Janet will back the bill*\"\n",
    "\n",
    "üéØ Goal: correct series of tags (**Janet/NNP will/MD back/VB the/DT bill/NN**)\n",
    "\n",
    "HMM is defiend by two tables\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-24%2012.06.37.png\" alt=\"Êà™Â±è2020-05-24 12.06.37\" style=\"zoom:50%;\" />\n",
    "\n",
    "- üëÜ Lists the $a_{ij}$ probabilities for transitioning betweeen the hidden states (POS tags)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-24%2012.08.01.png\" alt=\"Êà™Â±è2020-05-24 12.08.01\" style=\"zoom:50%;\" />\n",
    "\n",
    "- üëÜ Expresses the $b_i(o_t)$ probabilities, the *observation* likelihood s of words given tags\n",
    "  - This table is (slightly simplified) from counts in WSJ corpus\n",
    "\n",
    "Computation:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-24%2012.10.53.png\" alt=\"Êà™Â±è2020-05-24 12.10.53\" style=\"zoom:40%;\" />\n",
    "\n",
    "- There're $N=5$ state columns\n",
    "- begin in column 1 (for the word *Janet*) by setting the Viterbi value in each cell to the product of \n",
    "  - the $\\pi$ transistion probability (the start probability for that state $i$, which we get from the <*s*> entry), and \n",
    "  - the observation likelihood of the word *Janet* givne the tag for that cell\n",
    "    - Most of the cells in the column are zero since the word *Janet* cannot be any of those tags (See Figure 8.8 above)\n",
    "- Next, each cell in the *will* column gets update\n",
    "  - For each state, we compute the value $viterbi[s, t]$ by taking the maximum over the extensions of all the paths from the previous column that lead to the current cell\n",
    "  - Each cell gets the max of the 7 values from the previous column, multiplied by the appropriate transition probability\n",
    "    - Most of them are zero from the previous column in this case\n",
    "    - The remaining value is multiplied by the relevant observation probability, and the (trivial) max is taken. (In this case the final value, 2.772e-8, comes from the NNP state at the previous column. )\n",
    "\n",
    "### Example 2\n",
    "\n",
    "[HMM : Viterbi algorithm - a toy example](https://www.cis.upenn.edu/~cis262/notes/Example-Viterbi-DNA.pdf) üëç\n",
    "\n",
    "\n",
    "\n",
    "## Extending the HMM Algorithm to Trigrams\n",
    "\n",
    "In simple HMM model described above, the probability of a tag depends only on the previous tag \n",
    "$$\n",
    "P\\left(t_{1}^{n}\\right) \\approx \\prod_{i=1}^{n} P\\left(\\left.t_{i}\\right|_{i-1}\\right)\n",
    "$$\n",
    "In practice we use more of the history, letting the probability of a tag depend on the **two** previous tags\n",
    "$$\n",
    "P\\left(t_{1}^{n}\\right) \\approx \\prod_{i=1}^{n} P\\left(t_{i} | t_{i-1}, t_{i-2}\\right)\n",
    "$$\n",
    "\n",
    "- Small increase in performance (perhaps a half point)\n",
    "- But conditioning on two previous tags instead of one requires a significant change to the Viterbi algorithm ü§™\n",
    "  - For each cell, instead of taking a max over transitions from each cell in the previous column, we have to take a max over paths through the cells in the previous two columns\n",
    "  - thus considering $N^2$ rather than $N$ hidden states at every observation.\n",
    "\n",
    "In addition to increasing the context window, HMM taggers have a number of other advanced features\n",
    "\n",
    "-  let the tagger know the location of the end of the sentence by <span style=\"color:blue\">adding dependence on an end-of-sequence marker </span>for $t_{n+1}$ \n",
    "   $$\n",
    "   \\hat{t}_{1}^{n}=\\underset{t_{1}^{n}}{\\operatorname{argmax}} P\\left(t_{1}^{n} | w_{1}^{n}\\right) \\approx \\underset{t_{1}^{n}}{\\operatorname{argmax}}\\left[\\prod_{i=1}^{n} P\\left(w_{i} | t_{i}\\right) P\\left(t_{i} | t_{i-1}, t_{i-2}\\right)\\right] \\color{blue}{P\\left(t_{n+1} | t_{n}\\right)}\n",
    "   $$\n",
    "\n",
    "   - Three of the tags ($t_{-1}, t_0, t_{n+1}$) used in the context will fall off the edge of the sentence, and hence will not match regular words\n",
    "     - These tags can all be set to be a single special ‚Äòsentence boundary‚Äô tag that is added to the tagset, which assumes sentences boundaries have already been marked.\n",
    "\n",
    "üî¥ Problem with trigram taggers: <span style=\"color:red\">data sparsity</span>\n",
    "\n",
    "- Any particular sequence of tags $t_{i-2}, t_{i-1}, t_{i}$ that occurs in the test set may simply never have occurred in the training set.\n",
    "\n",
    "- Therefore we can NOT compute the tag trigram probability just by the maximum likelihood estimate from counts, following\n",
    "  $$\n",
    "  P\\left(t_{i} | t_{i-1}, t_{i-2}\\right)=\\frac{C\\left(t_{i-2}, t_{i-1}, t_{i}\\right)}{C\\left(t_{i-2}, t_{i-1}\\right)}\n",
    "  $$\n",
    "\n",
    "  - Many of these counts will be zero in any training set, and we will incorrectly predict that a given tag sequence will never occur!\n",
    "\n",
    "- We need a way to estimate $P(t_i|t_{i-1}, t_{i-2})$ even if the sequence $t_{i-2}, t_{i-1}, t_i$ never occurs in the training data\n",
    "\n",
    "  - Standard approach: estimate the probability by combining more robust, but weaker estimators.\n",
    "\n",
    "  - E.g., if we‚Äôve never seen the tag sequence PRP VB TO, and so can‚Äôt compute $P(\\mathrm{TO} | \\mathrm{PRP}, \\mathrm{VB})$ from this frequency, we still could rely on the bigram probability $P(\\mathrm{TO} | \\mathrm{VB})$, or even the unigram probability $P(\\mathrm{TO})$.\n",
    "\n",
    "    The maximum likelihood estimation of each of these probabilities can be computed from a corpus with the following counts:\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\text { Trigrams } \\qquad \\hat{P}\\left(t_{i} | t_{i-1}, t_{i-2}\\right) &=\\frac{C\\left(t_{i-2}, t_{i-1}, t_{i}\\right)}{C\\left(t_{i-2}, t_{i-1}\\right)} \\\\\n",
    "    \\text { Bigrams } \\qquad \\hat{P}\\left(t_{i} | t_{i-1}\\right) &=\\frac{C\\left(t_{i-1}, t_{i}\\right)}{C\\left(t_{i-1}\\right)} \\\\\n",
    "    \\text { Unigrams } \\qquad \\hat{P}\\left(t_{i}\\right)&=\\frac{C\\left(t_{i}\\right)}{N}\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "    We use **linear interpolation** to combine these  three estimators to estimate the trigram probability $P\\left(t_{i} | t_{i-1}, t_{i-2}\\right)$. We estimate the probablity $P\\left(t_{i} | t_{i-1}, t_{i-2}\\right)$ by a **weighted sum** of the unigram, bigram, and trigram probabilities\n",
    "    $$\n",
    "    P\\left(t_{i} | t_{i-1} t_{i-2}\\right)=\\lambda_{3} \\hat{P}\\left(\\left.t_{i}\\right|_{i-1} t_{i-2}\\right)+\\lambda_{2} \\hat{P}\\left(t_{i} | t_{i-1}\\right)+\\lambda_{1} \\hat{P}\\left(t_{i}\\right)\n",
    "    $$\n",
    "\n",
    "    - $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$\n",
    "    - $\\lambda$s are set by **deleted interpolation**\n",
    "      - successively delete each trigram from the training corpus and choose the Œªs so as to maximize the likelihood of the rest of the corpus.\n",
    "        - helps to set the Œªs in such a way as to generalize to unseen data and not overfit üëç\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-26%2011.34.37.png\" alt=\"Êà™Â±è2020-05-26 11.34.37\" style=\"zoom: 70%;\" />\n",
    "\n",
    "\n",
    "\n",
    "## Beam Search\n",
    "\n",
    "<span style=\"color:red\">Problem of vanilla Viterbi algorithms</span>\n",
    "\n",
    "- Slow, when the number of states grows very large\n",
    "- Complexity: $O(N^2T)$\n",
    "  - $N$: Number of states\n",
    "    - Can be large for trigram taggers\n",
    "      - E.g.: Considering very previous pair of the 45 tags resulting in $45^3=91125$ computations per column!!! üò±\n",
    "    - can be even larger for other applications of Viterbi (E.g., decoding in neural networks)\n",
    "\n",
    "üîß Common solution: **beam search decoding**\n",
    "\n",
    "- üí° Instead of keeping the entire column of states at each time point $t$, we just keep the best few hypothesis at that point.\n",
    "\n",
    "  At time $t$:\n",
    "\n",
    "  1. Compute the Viterbi score for each of the $N$ cells\n",
    "  2. Sort the scores\n",
    "  3. Keep only the best-scoring states. The rest are pruned out and NOT continued forward to time $t+1$\n",
    "\n",
    "- Implementation\n",
    "\n",
    "  - Keep a fixed number of states (beam width) $\\beta$ instead of all $N$ current states\n",
    "\n",
    "  - Alternatively $\\beta$ can be modeled as a fixed percentage of the $N$ states, or as a probability threshold\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-26%2011.54.17.png\" alt=\"Êà™Â±è2020-05-26 11.54.17\" style=\"zoom:70%;\" />\n",
    "\n",
    "\n",
    "\n",
    "## Unknown Words\n",
    "\n",
    "One useful feature for distinguishing parts of speech is **word shape**\n",
    "\n",
    "- words starting with capital letters are likely to be proper nouns (NNP).\n",
    "\n",
    "Strongest source of information for guessing the part-of-speech of unknown words: **morphology**\n",
    "\n",
    "- Words ending in *-s* $\\Rightarrow$ plural nouns (NNS)\n",
    "- words ending with *-ed* $\\Rightarrow$ past participles (VBN)\n",
    "- words ending with *-able* $\\Rightarrow$ adjectives (JJ)\n",
    "\n",
    "- ...\n",
    "\n",
    "We store for each *suffixes* of up to 10 letters the statistics of the tag it was associated with in training. We are thus computing for each suffix of length $i$ the probability of the tag $t_i$ given the suffix letters\n",
    "$$\n",
    "P\\left(t_{i} | l_{n-i+1} \\ldots l_{n}\\right)\n",
    "$$\n",
    "\n",
    "**Back-off** is used to smooth these probabilities with successively shorter suffixes.\n",
    "\n",
    "Unknown words are unlikely to be closed-class words (like prepositions), suffix probabilites can be computed only for words whose training set frequency is $\\leq 10$, or only for open-class words.\n",
    "\n",
    "As $P\\left(t_{i} | l_{n-i+1} \\ldots l_{n}\\right)$ gives a posterior estimate $p(t_i|w_i)$, we can compute the likelihood $p(w_i|t_i)$ tha HMMs require by using Bayesian inversion (i.e., using Bayes‚Äô rule and computation of the two priors $P(t_i)$ and $P(t_i|l_{n-i+1}\\dots l_n)$).\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [Speech and Language Processing, ch8](https://web.stanford.edu/~jurafsky/slp3/8.pdf)\n",
    "\n",
    "- Viterbi:\n",
    "  - https://www.zhihu.com/question/20136144 \n",
    "  - Example: [HMM : Viterbi algorithm - a toy example]("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
